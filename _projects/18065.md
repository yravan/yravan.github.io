---
layout: page
title: Pruning Neural Networks with Matrix Methods
description: May 2024
img: /assets/18065/header.png
importance: 1
category: fun
related_publications: true
---

Neural Networks are the state-of-the-art models for tasks such as classification, regression, generative modeling, etc, yet most networks are very large (AlexNet (240Mb, 21 M parameters), ChatGPT (500Gb, 175 B parameters), etc.). 

This leads to high inference time, necessitated lots of computational power, and limits applications in edge computing. Pruning aims to reduce redundant parameters, and we approach this using simple linear algebra techniques.

This project was done for 18.065 (Matrix Methods) & the code can be found [here](https://github.com/yravan/neural_network_pruning)

<div class="row">
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
    <div class="col-sm-10 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="/assets/18065/methods.png" title="methods" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
</div>
<div class="caption">
    Methods
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="/assets/18065/randomized_multiplication_algorithm.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="/assets/18065/PCA_algorithm.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="/assets/18065/Low_Rank_algorithm.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Algorithm Psuedocode
</div>


Our experiments indicated that:
-  Randomized multiplication does not seem to improve
the performance of our fully connected networks. The over-
head of drawing samples and the number of samples needed
to achieve good performance causes the inference time of
the randomized network to be significantly larger than the
unmodified network
- PCA pruning is quite effective for sub-
stantially improving model performance without a substan-
tial loss in accuracy. The theoretical effectiveness is best
realized for larger networks where multiplication is a bottle-
neck.
- Low-rank approximation pruning is quite
effective for substantially improving model performance
without a substantial loss in accuracy. The theoretical effec-
tiveness is best realized for larger networks where multipli-
cation is a bottleneck.
-  These methods are not particularly useful
for CNNs, due to computational overhead and practical
differences in computing power.

More in-depth analysis and results can be found below.


## [Code](https://github.com/yravan/neural_network_pruning)

## [Project Presentation](/assets/18065/Presentation-2.pdf):
<div class="row">
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
    <div class="col-sm-10 mt-3 mt-md-0">
        <iframe src="/assets/18065/Presentation-2.pdf" width="100%" height="600px"></iframe>
    </div>
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
</div>

## [Project Report](/assets/18065/icml2024-2.pdf):
<div class="row">
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
    <div class="col-sm-10 mt-3 mt-md-0">
        <iframe src="/assets/18065/icml2024-2.pdf" width="100%" height="600px"></iframe>
    </div>
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
</div>





